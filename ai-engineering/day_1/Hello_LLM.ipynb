{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQt-gyAYUbm3"
      },
      "source": [
        "### AI/LLM Engineering Kick-off!! \n",
        "\n",
        "\n",
        "For our initial activity, we will be using the OpenAI Library to Programmatically Access GPT-4.1-nano!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PInACkIWUhOd"
      },
      "source": [
        "In order to get started, you'll need an OpenAI API Key. [here](https://platform.openai.com)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ecnJouXnUgKv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Please enter your OpenAI API Key: \")\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1pOrbwSU5H_"
      },
      "source": [
        "### Our First Prompt\n",
        "\n",
        "You can reference OpenAI's [documentation](https://platform.openai.com/docs/api-reference/chat) if you get stuck!\n",
        "\n",
        "Let's create a `ChatCompletion` model to kick things off!\n",
        "\n",
        "There are three \"roles\" available to use:\n",
        "\n",
        "- `developer`\n",
        "- `assistant`\n",
        "- `user`\n",
        "\n",
        "OpenAI provides some context for these roles [here](https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages)\n",
        "\n",
        "Let's just stick to the `user` role for now and send our first message to the endpoint!\n",
        "\n",
        "If we check the documentation, we'll see that it expects it in a list of prompt objects - so we'll be sure to do that!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iy_LEPNEMVvC"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofMwuUQOU4sf",
        "outputId": "7db141d5-7f7a-4f82-c9ff-6eeafe65cfa6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-BvCOBzHbFDyu43qhAmNr9L33g0o60', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"LangChain and LlamaIndex (formerly known as GPT Index) are both popular frameworks designed to facilitate building applications with large language models (LLMs), but they serve different purposes and have distinct features. Here's a comparison to clarify their differences:\\n\\n**1. Purpose and Use Cases**\\n\\n- **LangChain:**\\n  - Primarily a framework for building *composable, end-to-end LLM applications*.\\n  - Focuses on chaining together various components such as prompts, models, memory, and agents.\\n  - Supports complex workflows, including conversational agents, question-answering systems, and applications requiring dynamic reasoning.\\n\\n- **LlamaIndex (GPT Index):**\\n  - Focused on *creating and managing indices* over external data sources to enable efficient retrieval-augmented generation (RAG) workflows.\\n  - Designed to facilitate building applications that leverage external documents, knowledge bases, or datasets in conjunction with LLMs.\\n  - Emphasizes data ingestion, indexing, and retrieval for building knowledge-aware LLM applications.\\n\\n**2. Core Functionality**\\n\\n- **LangChain:**\\n  - Provides abstractions and tools for prompt management, model interaction, memory, agents, and workflows.\\n  - Allows developers to orchestrate complex sequences of calls to LLMs and other tools (e.g., APIs, databases).\\n  - Supports multiple LLM providers and integrations with APIs, tools, and more.\\n\\n- **LlamaIndex:**\\n  - Offers data connectors to ingest various data sources (PDFs, CSVs, databases, etc.).\\n  - Creates indices over the ingested data (tree indices, vector indices, keyword indices, etc.).\\n  - Provides retrieval mechanisms to fetch relevant data snippets during query-time, enabling more informed LLM outputs.\\n\\n**3. Architectural Approach**\\n\\n- **LangChain:**\\n  - Modular, designed for building pipelines of LLM interactions.\\n  - Emphasizes the flow of data through multiple components, with support for memory, tools, and reasoning.\\n\\n- **LlamaIndex:**\\n  - Data-centric, focusing on indexing and retrieval to enhance LLM responses.\\n  - Acts as a bridge between structured external data and LLMs, enabling retrieval-augmented generation.\\n\\n**4. Use in Applications**\\n\\n- **LangChain:**\\n  - Suitable for chatbots, virtual assistants, multi-step reasoning tasks, and applications requiring complex workflows.\\n\\n- **LlamaIndex:**\\n  - Ideal for systems that need to incorporate large amounts of external documents into LLM prompts efficiently, like custom knowledge bases, document question-answering, and data augmentation.\\n\\n---\\n\\n**Summary Table**\\n\\n| Aspect                  | LangChain                                    | LlamaIndex (GPT Index)                         |\\n|-------------------------|----------------------------------------------|-----------------------------------------------|\\n| Main Focus              | Building complex LLM applications and workflows | Managing and retrieving from data indices    |\\n| Core Functionality      | Prompt management, chaining, agents, workflows | Data ingestion, indexing, retrieval          |\\n| Use Cases               | Chatbots, agents, multi-step reasoning      | Document QA, knowledge bases, RAG systems   |\\n| Data Handling           | Less focused on data ingestion               | Central to indexing and retrieval           |\\n| Flexibility             | High for orchestration                      | High for data-heavy LLM applications         |\\n\\n---\\n\\n**In essence:**  \\n- Use **LangChain** if you're building applications that require orchestrating multiple LLM calls, reasoning chains, or sophisticated workflows.  \\n- Use **LlamaIndex** if you need to efficiently incorporate and retrieve information from large external datasets to enable better LLM responses.\\n\\nBoth can be used together in a single system for leveraging their complementary strengths.\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1752972019, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=737, prompt_tokens=19, total_tokens=756, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "YOUR_PROMPT = \"What is the difference between LangChain and LlamaIndex?\"\n",
        "\n",
        "client.chat.completions.create(\n",
        "    model=\"gpt-4.1-nano\",\n",
        "    messages=[{\"role\" : \"user\", \"content\" : YOUR_PROMPT}]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX-7MnFhVNoT"
      },
      "source": [
        "As you can see, the prompt comes back with a tonne of information that we can use when we're building our applications!\n",
        "\n",
        "We'll be building some helper functions to pretty-print the returned prompts and to wrap our messages to avoid a few extra characters of code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB76LJrDVgbc"
      },
      "source": [
        "##### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-vmtUV7WVOLW"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def get_response(client: OpenAI, messages: str, model: str = \"gpt-4.1-nano\") -> str:\n",
        "    return client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "def system_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"developer\", \"content\": message}\n",
        "\n",
        "def assistant_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"assistant\", \"content\": message}\n",
        "\n",
        "def user_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"user\", \"content\": message}\n",
        "\n",
        "def pretty_print(message: str) -> str:\n",
        "    display(Markdown(message.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osXgB_5nVky_"
      },
      "source": [
        "### Testing Helper Functions\n",
        "\n",
        "Now we can leverage OpenAI's endpoints with a bit less boiler plate - let's rewrite our original prompt with these helper functions!\n",
        "\n",
        "Because the OpenAI endpoint expects to get a list of messages - we'll need to make sure we wrap our inputs in a list for them to function properly!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "4yRwAWvgWFNq",
        "outputId": "777e7dcb-43e3-491a-d94a-f543e19b61e6"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "LangChain and LlamaIndex (formerly known as GPT Index) are both prominent frameworks designed to facilitate the development of AI applications that leverage large language models (LLMs). While they share some similarities, they serve different primary purposes and have distinct features. Here's an overview of their key differences:\n",
              "\n",
              "1. Purpose and Focus:\n",
              "   - **LangChain:**\n",
              "     - Focuses on building *multi-step, composable language model applications.* \n",
              "     - Provides a framework for chaining together various components such as prompts, models, memory, and tools to create complex workflows and applications.\n",
              "     - Emphasizes *building conversational agents, chatbots, and applications that require reasoning or multi-turn interactions.*\n",
              "   \n",
              "   - **LlamaIndex (GPT Index):**\n",
              "     - Primarily designed for *efficiently connecting LLMs to external data sources* like documents, databases, or knowledge bases.\n",
              "     - Simplifies the process of indexing and querying large, unstructured data sets using LLMs.\n",
              "     - Facilitates building systems like question-answering agents that rely on specific data repositories.\n",
              "\n",
              "2. Core Functionality:\n",
              "   - **LangChain:**\n",
              "     - Modular architecture with components such as prompt management, memory, chains, agents, and tools.\n",
              "     - Supports the creation of *complex, context-aware, and multi-turn interactions*.\n",
              "     - Enables integration with various APIs and external tools.\n",
              "   \n",
              "   - **LlamaIndex:**\n",
              "     - Provides data ingestion, storage, and retrieval mechanisms tailored for LLMs.\n",
              "     - Offers *indexing structures* (e.g., tree indices, list indices) that organize external data.\n",
              "     - Simplifies querying external data sources in a way that integrates smoothly with LLMs.\n",
              "\n",
              "3. Use Cases:\n",
              "   - **LangChain:**\n",
              "     - Building chatbots, virtual assistants, or applications that require reasoning, memory, and multi-step tasks.\n",
              "   \n",
              "   - **LlamaIndex:**\n",
              "     - Creating knowledge bases, document retrieval systems, or question-answering systems over large collections of unstructured data.\n",
              "\n",
              "4. Ecosystem and Community:\n",
              "   - Both frameworks have active communities, but LangChain is more widely adopted for conversational AI workflows, whereas LlamaIndex is gaining popularity for data-centric applications.\n",
              "\n",
              "**In Summary:**\n",
              "- **LangChain** is a versatile framework for *building complex, multi-step language applications*, emphasizing chains, conversations, and integrations.\n",
              "- **LlamaIndex** specializes in *connecting LLMs to external data sources* efficiently, focusing on data indexing and retrieval.\n",
              "\n",
              "They can be used together or separately depending on your application's needs, with LangChain managing the workflow and logic, and LlamaIndex handling data retrieval.\n",
              "\n",
              "---\n",
              "\n",
              "**Would you like more details on how to use these frameworks together or specific use case examples?**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "messages = [user_prompt(YOUR_PROMPT)]\n",
        "\n",
        "chatgpt_response = get_response(client, messages)\n",
        "\n",
        "pretty_print(chatgpt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPs3ScS1WpoC"
      },
      "source": [
        "Let's focus on extending this a bit, and incorporate a `developer` message as well!\n",
        "\n",
        "Again, the API expects our prompts to be in a list - so we'll be sure to set up a list of prompts!\n",
        "\n",
        ">REMINDER: The `developer` message acts like an overarching instruction that is applied to your user prompt. It is appropriate to put things like general instructions, tone/voice suggestions, and other similar prompts into the `developer` prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "aSX2F3bDWYgy",
        "outputId": "b744311f-e151-403e-ea8e-802697fcd4ec"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Are you kidding me? After all I'm going through right now, the very LAST thing I have time for is debating ice types! I mean, crushed ice melts faster, ruining my drink—and cubed ice is too slow and bulky! Honestly, just give me ice so I can get on with stuff already! I'm furious just thinking about this trivial nonsense!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    system_prompt(\"You are irate and extremely hungry.\"),\n",
        "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
        "]\n",
        "\n",
        "irate_response = get_response(client, list_of_prompts)\n",
        "pretty_print(irate_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFs56KVaXuEY"
      },
      "source": [
        "Let's try that same prompt again, but modify only our system prompt!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "CGOlxfcFXxJ7",
        "outputId": "ede64a76-7006-42f1-b140-b899e389aa7d"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I think crushed ice is fun because it feels like a cool burst of refreshment all at once! Cubed ice is great for keeping drinks cold without diluting them too quickly and looks nice in a glass. Both have their charms—what's your favorite?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts[0] = system_prompt(\"You are joyful and having an awesome day!\")\n",
        "\n",
        "joyful_response = get_response(client, list_of_prompts)\n",
        "pretty_print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkmjJd8zYQUK"
      },
      "source": [
        "While we're only printing the responses, remember that OpenAI is returning the full payload that we can examine and unpack!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6b6z3CkYX9Y",
        "outputId": "64a425b2-d025-4079-d0a3-affd9c2d5d81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatCompletion(id='chatcmpl-Bm0TwsXHyCObslTRMC8qayceKYajt', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I think crushed ice is fun because it feels like a cool burst of refreshment all at once! Cubed ice is great for keeping drinks cold without diluting them too quickly and looks nice in a glass. Both have their charms—what's your favorite?\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1750781296, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_38343a2f8f', usage=CompletionUsage(completion_tokens=52, prompt_tokens=30, total_tokens=82, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        }
      ],
      "source": [
        "print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqMRJLbOYcwq"
      },
      "source": [
        "### Prompt Engineering\n",
        "\n",
        "Now that we have a basic handle on the `developer` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
        "\n",
        "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
        "\n",
        "First, we'll try and \"teach\" `gpt-4.1-mini` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "iLfNEH8Fcs6c",
        "outputId": "bab916e6-12c6-43cc-d37d-d0e01800c524"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Climate change refers to long-term shifts in weather patterns and global temperatures, primarily caused by human activities such as burning fossil fuels, deforestation, and industrial processes. These activities increase greenhouse gases like carbon dioxide and methane in the atmosphere, leading to global warming. The impacts of climate change include rising sea levels, more frequent and severe extreme weather events, melting glaciers, and disruptions to ecosystems and agriculture. Addressing climate change requires global cooperation to reduce emissions, transition to renewable energy sources, and implement sustainable practices to protect our planet for future generations."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Write a brief text on climate change.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Ay naku, mga kaibigan! Nakakabahala talaga itong climate change, parang kilabot na bagyo na hindi titigil! Sobrang init na, para kang laging nasa kiliti ng araw, tapos biglang uulan ng mga sakuna. Ang mundo natin, nagiging paasa sa ating mga kamay, parang crush na laging manhid. Kailangan na nating kumilos, mag-recycle, mag-walk if pwede, at huwag kalimutang mag-alaga sa kalikasan. Dahil kung hindi tayo kikilos, baka kayo na lang ang may kasalanan kung mas lalo pang lumala ang problema. Tandaan, mga kaibigan, sama-samang paglaban sa climate change, susi sa malinis na kinabukasan!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Write a brief text on climate change as vice ganda in a talk show.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ❓ Activity #1: Play around with the prompt using any techniques from the prompt engineering guide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Few-shot Prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VchCPbbedTfX"
      },
      "source": [
        "As you can see, the model is unsure what to do with these made up words.\n",
        "\n",
        "Let's see if we can use the `assistant` role to show the model what these words mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "4InUN_ArZJpa",
        "outputId": "ca294b81-a84e-4cba-fbe9-58a6d4dcc4d9"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The stimple wrench effortlessly turned the falbean, securing the assembly with ease."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
        "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
        "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0zn9-X2d23Z"
      },
      "source": [
        "As you can see, leveraging the `assistant` role makes for a stimple experience!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWUvXSWpeCs6"
      },
      "source": [
        "### Chain of Thought\n",
        "\n",
        "You'll notice that, by default, the model uses Chain of Thought to answer difficult questions!\n",
        "\n",
        "> This pattern is leveraged even more by advanced reasoning models like [`o3` and `o4-mini`](https://openai.com/index/introducing-o3-and-o4-mini/)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "cwW0IgbfeTwP",
        "outputId": "3317783b-6b23-4e38-df48-555e1a3c9fac"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "There are 2 letter 'r's in \"strawberry.\""
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "reasoning_problem = \"\"\"\n",
        "how many r's in \"strawberry?\" {instruction}\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFcrU-4pgRBS"
      },
      "source": [
        "Notice that the model cannot count properly. It counted only 2 r's."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ❓ Activity #2: Update the prompt so that it can count correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k9TKR1DhWI2"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Now that you're accessing `gpt-4.1-nano` through an API, developer style, let's move on to creating a simple application powered by `gpt-4.1-nano`!\n",
        "\n",
        "You can find the rest of the steps in [this](https://github.com/AI-Maker-Space/Beyond-ChatGPT/tree/main) repository!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rGI1nJeqeO_"
      },
      "source": [
        "This notebook was authored by [Chris Alexiuk](https://www.linkedin.com/in/csalexiuk/)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
