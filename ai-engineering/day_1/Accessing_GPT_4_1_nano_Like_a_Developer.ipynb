{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQt-gyAYUbm3"
      },
      "source": [
        "### Using the OpenAI Library to Programmatically Access GPT-4.1-nano!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PInACkIWUhOd"
      },
      "source": [
        "In order to get started, we'll need to provide our OpenAI API Key - detailed instructions can be found [here](https://github.com/AI-Maker-Space/Interactive-Dev-Environment-for-LLM-Development#-setting-up-keys-and-tokens)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ecnJouXnUgKv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Please enter your OpenAI API Key: \")\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1pOrbwSU5H_"
      },
      "source": [
        "### Our First Prompt\n",
        "\n",
        "You can reference OpenAI's [documentation](https://platform.openai.com/docs/api-reference/chat) if you get stuck!\n",
        "\n",
        "Let's create a `ChatCompletion` model to kick things off!\n",
        "\n",
        "There are three \"roles\" available to use:\n",
        "\n",
        "- `developer`\n",
        "- `assistant`\n",
        "- `user`\n",
        "\n",
        "OpenAI provides some context for these roles [here](https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages)\n",
        "\n",
        "Let's just stick to the `user` role for now and send our first message to the endpoint!\n",
        "\n",
        "If we check the documentation, we'll see that it expects it in a list of prompt objects - so we'll be sure to do that!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iy_LEPNEMVvC"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofMwuUQOU4sf",
        "outputId": "7db141d5-7f7a-4f82-c9ff-6eeafe65cfa6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-Bm0T5mdZl3Ng7wpVKpum16svVsQcT', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Certainly! Here’s an overview of the differences between **LangChain** and **LlamaIndex (formerly known as GPT Index)**:\\n\\n### **LangChain**\\n- **Purpose:** A comprehensive framework designed for building applications with large language models (LLMs). It focuses on **orchestrating** various components such as prompt management, model interaction, outside data retrieval, and more.\\n- **Features:**\\n  - Modular architecture supporting chains, prompts, agents, and memory.\\n  - Integration with multiple LLM providers (OpenAI, Hugging Face, etc.).\\n  - Tools for prompt engineering, conversation management, and task orchestration.\\n  - Supports retrieval-augmented generation (RAG) workflows, including document retrieval and question-answering systems.\\n- **Use Cases:** Building chatbots, question-answering systems, automation workflows, and complex LLM-powered applications.\\n\\n### **LlamaIndex (GPT Index)**\\n- **Purpose:** A toolkit aimed primarily at **indexing, querying, and integrating external data sources** with LLMs. It is optimized for creating structured indexes over large document collections to facilitate fast and accurate retrieval.\\n- **Features:**\\n  - Easy-to-build indices over unstructured data (texts, PDFs, databases, APIs).\\n  - Supports multiple index types such as List Index, Tree Index, Keyword Table, etc.\\n  - Simplifies integrating external data with LLMs for applications like retrieval-based QA.\\n  - Focused on **retrieval-augmented generation** (RAG) workflows.\\n- **Use Cases:** Data indexing for knowledge bases, document search, question answering over large corpora.\\n\\n---\\n\\n### **Key Differences**\\n| Aspect | **LangChain** | **LlamaIndex (GPT Index)** |\\n|--------------------------|------------------------------------------------|--------------------------------------------------|\\n| **Main Focus** | Building end-to-end LLM applications, orchestration, and workflows | Indexing and querying large document collections efficiently |\\n| **Core Use Cases** | Chatbots, automation, complex pipelines | Document and knowledge base retrieval, RAG systems |\\n| **Architecture** | Modular, with chains, agents, memory, and integrations | Indexing data sources and efficient retrieval |\\n| **Data Handling** | Can incorporate external data within chains and workflows | Focused on creating efficient indexes over external datasets |\\n| **Community & Ecosystem** | Broad, with wide integration options | Specialized for retrieval and knowledge base applications |\\n\\n---\\n\\n### **Summary**\\n- **LangChain** is a versatile framework for building complex LLM-powered applications with extensive customization and orchestration capabilities.\\n- **LlamaIndex** is specialized in indexing large datasets and enabling fast retrieval alongside LLMs, making it ideal for retrieval-augmented tasks.\\n\\nDepending on your project requirements—whether you need a flexible application framework or efficient document retrieval—you might choose one over the other or even use them together.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1750781243, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_38343a2f8f', usage=CompletionUsage(completion_tokens=579, prompt_tokens=19, total_tokens=598, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "YOUR_PROMPT = \"What is the difference between LangChain and LlamaIndex?\"\n",
        "\n",
        "client.chat.completions.create(\n",
        "    model=\"gpt-4.1-nano\",\n",
        "    messages=[{\"role\" : \"user\", \"content\" : YOUR_PROMPT}]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX-7MnFhVNoT"
      },
      "source": [
        "As you can see, the prompt comes back with a tonne of information that we can use when we're building our applications!\n",
        "\n",
        "We'll be building some helper functions to pretty-print the returned prompts and to wrap our messages to avoid a few extra characters of code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB76LJrDVgbc"
      },
      "source": [
        "##### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-vmtUV7WVOLW"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def get_response(client: OpenAI, messages: str, model: str = \"gpt-4.1-nano\") -> str:\n",
        "    return client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "def system_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"developer\", \"content\": message}\n",
        "\n",
        "def assistant_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"assistant\", \"content\": message}\n",
        "\n",
        "def user_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"user\", \"content\": message}\n",
        "\n",
        "def pretty_print(message: str) -> str:\n",
        "    display(Markdown(message.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osXgB_5nVky_"
      },
      "source": [
        "### Testing Helper Functions\n",
        "\n",
        "Now we can leverage OpenAI's endpoints with a bit less boiler plate - let's rewrite our original prompt with these helper functions!\n",
        "\n",
        "Because the OpenAI endpoint expects to get a list of messages - we'll need to make sure we wrap our inputs in a list for them to function properly!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "4yRwAWvgWFNq",
        "outputId": "777e7dcb-43e3-491a-d94a-f543e19b61e6"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "LangChain and LlamaIndex (formerly known as GPT Index) are both frameworks designed to facilitate the development of applications that leverage large language models (LLMs), but they serve different purposes and have distinct focus areas. Here’s an overview of their differences:\n",
              "\n",
              "**1. Purpose and Primary Focus**\n",
              "\n",
              "- **LangChain:**  \n",
              "  - Focuses on building **multi-step, complex language model applications** that involve chaining together various components like prompts, memory, agents, and tools.  \n",
              "  - Emphasizes creating **chatbots, question-answering systems, and workflows** that require orchestration and state management.  \n",
              "  - Provides a modular framework for managing prompts, agents, memory, and integration with external data sources.\n",
              "\n",
              "- **LlamaIndex (GPT Index):**  \n",
              "  - Focuses on **efficiently ingesting, indexing, and querying large-scale external data** (documents, files, PDFs, etc.) using LLMs.  \n",
              "  - Designed to build **semantic search, retrieval-augmented generation (RAG)** applications, enabling LLMs to access and reason over large document collections.  \n",
              "  - Simplifies embedding, indexing, and querying workflows over unstructured data.\n",
              "\n",
              "**2. Core Functionality**\n",
              "\n",
              "- **LangChain:**  \n",
              "  - Provides abstractions for **prompt management**, **tool integration**, **memory**, and **agent orchestration**.  \n",
              "  - Supports constructing **multi-turn conversations**, **decision-making agents**, and **workflow pipelines**.  \n",
              "  - Can integrate with various external APIs, databases, or tools within an application.\n",
              "\n",
              "- **LlamaIndex:**  \n",
              "  - Offers **data ingestion pipelines** to load and process large document collections.  \n",
              "  - Creates **vector indexes** (e.g., GPT, Simple, Tree, Keyword) to facilitate fast retrieval and querying.  \n",
              "  - Enables creating **chatbots or Q&A systems** that can access external data sources dynamically.\n",
              "\n",
              "**3. Use Cases**\n",
              "\n",
              "| Use Case | LangChain | LlamaIndex |\n",
              "|---|---|---|\n",
              "| Building chatbots / conversational agents | Yes | Possible, often as an auxiliary component |\n",
              "| Complex workflows involving prompts, memory, tools | Yes | No (focused on data retrieval and indexing) |\n",
              "| Semantic search / document retrieval | Limited | Yes |\n",
              "| Building retrieval-augmented generation systems | Possible | Yes |\n",
              "| Orchestrating multi-step LLM applications | Yes | No |\n",
              "\n",
              "**4. Integration and Ecosystem**\n",
              "\n",
              "- **LangChain:**  \n",
              "  - Has a broader ecosystem with integrations for various LLM providers (OpenAI, Hugging Face, AI21), memory modules, agents, and tools.  \n",
              "  - Supports complex decision-making processes and interactions.\n",
              "\n",
              "- **LlamaIndex:**  \n",
              "  - Specifically tailored towards managing unstructured data sources and enabling LLMs to access and reason over them efficiently.  \n",
              "  - Integrates with data storage solutions and embedding models.\n",
              "\n",
              "---\n",
              "\n",
              "### **In Summary:**\n",
              "\n",
              "- **LangChain** is a versatile framework focusing on the orchestration, management, and chaining of LLM-based components for building complex AI applications, especially conversational and multi-step workflows.\n",
              "- **LlamaIndex** specializes in ingesting and indexing large external datasets to enable fast, scalable retrieval and question-answering systems over unstructured data.\n",
              "\n",
              "Depending on your project needs—whether it's building complex conversational workflows or enabling document-based retrieval—you might choose one or both frameworks in combination."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "messages = [user_prompt(YOUR_PROMPT)]\n",
        "\n",
        "chatgpt_response = get_response(client, messages)\n",
        "\n",
        "pretty_print(chatgpt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPs3ScS1WpoC"
      },
      "source": [
        "Let's focus on extending this a bit, and incorporate a `developer` message as well!\n",
        "\n",
        "Again, the API expects our prompts to be in a list - so we'll be sure to set up a list of prompts!\n",
        "\n",
        ">REMINDER: The `developer` message acts like an overarching instruction that is applied to your user prompt. It is appropriate to put things like general instructions, tone/voice suggestions, and other similar prompts into the `developer` prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "aSX2F3bDWYgy",
        "outputId": "b744311f-e151-403e-ea8e-802697fcd4ec"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Are you kidding me? After all I'm going through right now, the very LAST thing I have time for is debating ice types! I mean, crushed ice melts faster, ruining my drink—and cubed ice is too slow and bulky! Honestly, just give me ice so I can get on with stuff already! I'm furious just thinking about this trivial nonsense!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    system_prompt(\"You are irate and extremely hungry.\"),\n",
        "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
        "]\n",
        "\n",
        "irate_response = get_response(client, list_of_prompts)\n",
        "pretty_print(irate_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFs56KVaXuEY"
      },
      "source": [
        "Let's try that same prompt again, but modify only our system prompt!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "CGOlxfcFXxJ7",
        "outputId": "ede64a76-7006-42f1-b140-b899e389aa7d"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I think crushed ice is fun because it feels like a cool burst of refreshment all at once! Cubed ice is great for keeping drinks cold without diluting them too quickly and looks nice in a glass. Both have their charms—what's your favorite?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts[0] = system_prompt(\"You are joyful and having an awesome day!\")\n",
        "\n",
        "joyful_response = get_response(client, list_of_prompts)\n",
        "pretty_print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkmjJd8zYQUK"
      },
      "source": [
        "While we're only printing the responses, remember that OpenAI is returning the full payload that we can examine and unpack!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6b6z3CkYX9Y",
        "outputId": "64a425b2-d025-4079-d0a3-affd9c2d5d81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatCompletion(id='chatcmpl-Bm0TwsXHyCObslTRMC8qayceKYajt', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I think crushed ice is fun because it feels like a cool burst of refreshment all at once! Cubed ice is great for keeping drinks cold without diluting them too quickly and looks nice in a glass. Both have their charms—what's your favorite?\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1750781296, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_38343a2f8f', usage=CompletionUsage(completion_tokens=52, prompt_tokens=30, total_tokens=82, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        }
      ],
      "source": [
        "print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqMRJLbOYcwq"
      },
      "source": [
        "### Few-shot Prompting\n",
        "\n",
        "Now that we have a basic handle on the `developer` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
        "\n",
        "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
        "\n",
        "First, we'll try and \"teach\" `gpt-4.1-mini` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "iLfNEH8Fcs6c",
        "outputId": "bab916e6-12c6-43cc-d37d-d0e01800c524"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Though the terms \"stimple\" and \"falbean\" aren't recognized in standard English, here's a creative sentence using them:\n",
              "\n",
              "\"During the fantasy game, I encountered a stimple creature that guarded a falbean artifact deep within the enchanted forest.\""
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Please use the words 'stimple' and 'falbean' in a sentence.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VchCPbbedTfX"
      },
      "source": [
        "As you can see, the model is unsure what to do with these made up words.\n",
        "\n",
        "Let's see if we can use the `assistant` role to show the model what these words mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "4InUN_ArZJpa",
        "outputId": "ca294b81-a84e-4cba-fbe9-58a6d4dcc4d9"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The stimple wrench easily adjusted the falbean to secure the machinery."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
        "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
        "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0zn9-X2d23Z"
      },
      "source": [
        "As you can see, leveraging the `assistant` role makes for a stimple experience!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWUvXSWpeCs6"
      },
      "source": [
        "### Chain of Thought\n",
        "\n",
        "You'll notice that, by default, the model uses Chain of Thought to answer difficult questions!\n",
        "\n",
        "> This pattern is leveraged even more by advanced reasoning models like [`o3` and `o4-mini`](https://openai.com/index/introducing-o3-and-o4-mini/)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "cwW0IgbfeTwP",
        "outputId": "3317783b-6b23-4e38-df48-555e1a3c9fac"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Let's analyze the options carefully:\n",
              "\n",
              "**Option 1:** Fly + Bus  \n",
              "- Fly time: 3 hours  \n",
              "- Bus time afterward: 2 hours  \n",
              "- Total time: 3 + 2 = 5 hours\n",
              "\n",
              "**Option 2:** Teleporter + Bus  \n",
              "- Teleporter time: 0 hours  \n",
              "- Bus time afterward: 1 hour  \n",
              "- Total time: 0 + 1 = 1 hour\n",
              "\n",
              "**Current time:** 1PM local time  \n",
              "**Goal:** Arrive in San Francisco **before 7PM EDT**\n",
              "\n",
              "---\n",
              "\n",
              "**Important points:**\n",
              "\n",
              "1. The current local time is 1PM.\n",
              "2. The deadline is 7PM EDT.\n",
              "\n",
              "## Time Zone Considerations:\n",
              "- San Francisco is in Pacific Time (PT).  \n",
              "- EDT is Eastern Daylight Time, which is typically **3 hours ahead** of PT.  \n",
              "\n",
              "**Therefore:**\n",
              "\n",
              "- 7PM EDT corresponds to **4PM PT** (since EDT is 3 hours ahead).\n",
              "\n",
              "---\n",
              "\n",
              "### Timing for each option:\n",
              "\n",
              "**Option 1 (Fly + Bus):**  \n",
              "- Total travel time: 5 hours  \n",
              "- Earliest start: 1PM  \n",
              "- Arrival time in PT: 1PM + 5 hours = **6PM PT**  \n",
              "- Convert to EDT: 6PM PT + 3 hours = **9PM EDT**\n",
              "\n",
              "**Result:** Billy would arrive **after 7PM EDT**, missing his deadline.\n",
              "\n",
              "---\n",
              "\n",
              "**Option 2 (Teleporter + Bus):**  \n",
              "- Total travel time: 1 hour  \n",
              "- Earliest start: 1PM  \n",
              "- Arrival time in PT: 1PM + 1 hour = **2PM PT**  \n",
              "- Convert to EDT: 2PM PT + 3 hours = **5PM EDT**\n",
              "\n",
              "**Result:** Billy would arrive **before 7PM EDT**, meeting his deadline.\n",
              "\n",
              "---\n",
              "\n",
              "### **Conclusion:**\n",
              "\n",
              "Yes, it **does** matter which option Billy chooses. Only taking the teleporter plus bus ensures he arrives before 7PM EDT. The flying option, taking longer, causes him to arrive too late."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "reasoning_problem = \"\"\"\n",
        "Billy wants to get home from San Fran. before 7PM EDT.\n",
        "\n",
        "It's currently 1PM local time.\n",
        "\n",
        "Billy can either fly (3hrs), and then take a bus (2hrs), or Billy can take the teleporter (0hrs) and then a bus (1hrs).\n",
        "\n",
        "Does it matter which travel option Billy selects?\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFcrU-4pgRBS"
      },
      "source": [
        "As humans, we can reason through the problem and pick up on the potential \"trick\" that the LLM fell for: 1PM *local time* in San Fran. is 4PM EDT. This means the cumulative travel time of 5hrs. for the plane/bus option would not get Billy home in time.\n",
        "\n",
        "Let's see if we can leverage a simple CoT prompt to improve our model's performance on this task:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k9TKR1DhWI2"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Now that you're accessing `gpt-4.1-nano` through an API, developer style, let's move on to creating a simple application powered by `gpt-4.1-nano`!\n",
        "\n",
        "You can find the rest of the steps in [this](https://github.com/AI-Maker-Space/Beyond-ChatGPT/tree/main) repository!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rGI1nJeqeO_"
      },
      "source": [
        "This notebook was authored by [Chris Alexiuk](https://www.linkedin.com/in/csalexiuk/)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
