{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQt-gyAYUbm3"
      },
      "source": [
        "### Using the OpenAI Library to Programmatically Access GPT-4.1-nano!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PInACkIWUhOd"
      },
      "source": [
        "In order to get started, we'll need to provide our OpenAI API Key - detailed instructions can be found [here](https://github.com/AI-Maker-Space/Interactive-Dev-Environment-for-LLM-Development#-setting-up-keys-and-tokens)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecnJouXnUgKv",
        "outputId": "c6c25850-395d-4cbf-9d26-bfe9253d1711"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Please enter your OpenAI API Key: \")\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1pOrbwSU5H_"
      },
      "source": [
        "### Our First Prompt\n",
        "\n",
        "You can reference OpenAI's [documentation](https://platform.openai.com/docs/api-reference/chat) if you get stuck!\n",
        "\n",
        "Let's create a `ChatCompletion` model to kick things off!\n",
        "\n",
        "There are three \"roles\" available to use:\n",
        "\n",
        "- `developer`\n",
        "- `assistant`\n",
        "- `user`\n",
        "\n",
        "OpenAI provides some context for these roles [here](https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages)\n",
        "\n",
        "Let's just stick to the `user` role for now and send our first message to the endpoint!\n",
        "\n",
        "If we check the documentation, we'll see that it expects it in a list of prompt objects - so we'll be sure to do that!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iy_LEPNEMVvC"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofMwuUQOU4sf",
        "outputId": "7db141d5-7f7a-4f82-c9ff-6eeafe65cfa6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-BUc2UgMuVcdtDkvmU1KdbRuq7z4bd', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Great question! LangChain and LlamaIndex (formerly known as GPT Index) are both popular frameworks designed to facilitate building applications with large language models (LLMs), but they serve different primary purposes and have distinct features.\\n\\n**LangChain:**\\n\\n- **Purpose:** A comprehensive framework for developing LLM-powered applications, especially those involving chaining multiple prompts, tools, and components.\\n- **Core Features:**\\n  - Supports building complex prompt workflows, including chaining, few-shot prompting, and memory.\\n  - Facilitates integration with various LLM providers (OpenAI, Hugging Face, etc.).\\n  - Enables the use of external tools and APIs within LLM applications.\\n  - Designed for creating chatbots, question-answering systems, and more complex AI workflows.\\n- **Use Cases:** Conversational agents, personalized assistants, multimodal workflows, and applications requiring advanced prompt engineering.\\n\\n---\\n\\n**LlamaIndex (GPT Index):**\\n\\n- **Purpose:** A toolkit focused on indexing, retrieving, and querying large document datasets efficiently using LLMs.\\n- **Core Features:**\\n  - Builds indices over document collections (e.g., PDFs, text files, web pages).\\n  - Supports retrieval-augmented generation (RAG), where relevant documents are fetched to inform LLM responses.\\n  - Provides easy-to-use components for document ingestion, indexing, and querying.\\n  - Optimized for building applications that require knowledge base retrieval and question-answering over large external datasets.\\n- **Use Cases:** Document search, knowledge base creation, retrieval-augmented question answering, and information retrieval tasks.\\n\\n---\\n\\n### **Summary of the Main Difference:**\\n\\n| Aspect | LangChain | LlamaIndex (GPT Index) |\\n|---|---|---|\\n| **Primary Focus** | Building complex LLM workflows and applications | Efficient retrieval and querying of large document collections with LLMs |\\n| **Use Cases** | Chatbots, conversational AI, multi-step workflows | Knowledge bases, document QA, retrieval-augmented generation |\\n| **Features** | Prompt chaining, tool integration, memory management | Document indexing, fast retrieval, external knowledge integration |\\n\\n---\\n\\n**In essence:**  \\n- Use **LangChain** if you want to build sophisticated, multi-step applications with LLMs that may involve tools, memory, and layered prompts.  \\n- Use **LlamaIndex** if your goal is to index large amounts of documents and enable efficient retrieval and question-answering over them, often in a retrieval-augmented setup.\\n\\nBoth can be complementary; some projects utilize both frameworks together for different parts of their architecture.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746635762, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_eede8f0d45', usage=CompletionUsage(completion_tokens=522, prompt_tokens=19, total_tokens=541, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "YOUR_PROMPT = \"What is the difference between LangChain and LlamaIndex?\"\n",
        "\n",
        "client.chat.completions.create(\n",
        "    model=\"gpt-4.1-nano\",\n",
        "    messages=[{\"role\" : \"user\", \"content\" : YOUR_PROMPT}]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX-7MnFhVNoT"
      },
      "source": [
        "As you can see, the prompt comes back with a tonne of information that we can use when we're building our applications!\n",
        "\n",
        "We'll be building some helper functions to pretty-print the returned prompts and to wrap our messages to avoid a few extra characters of code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB76LJrDVgbc"
      },
      "source": [
        "##### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-vmtUV7WVOLW"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def get_response(client: OpenAI, messages: str, model: str = \"gpt-4.1-nano\") -> str:\n",
        "    return client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "def system_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"developer\", \"content\": message}\n",
        "\n",
        "def assistant_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"assistant\", \"content\": message}\n",
        "\n",
        "def user_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"user\", \"content\": message}\n",
        "\n",
        "def pretty_print(message: str) -> str:\n",
        "    display(Markdown(message.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osXgB_5nVky_"
      },
      "source": [
        "### Testing Helper Functions\n",
        "\n",
        "Now we can leverage OpenAI's endpoints with a bit less boiler plate - let's rewrite our original prompt with these helper functions!\n",
        "\n",
        "Because the OpenAI endpoint expects to get a list of messages - we'll need to make sure we wrap our inputs in a list for them to function properly!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "4yRwAWvgWFNq",
        "outputId": "777e7dcb-43e3-491a-d94a-f543e19b61e6"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "LangChain and LlamaIndex (formerly known as GPT INDEX) are both prominent frameworks designed to facilitate the development of AI applications that leverage large language models (LLMs), but they serve different purposes and have distinct features. Here's a comparison to clarify their differences:\n",
              "\n",
              "**1. Purpose and Focus:**\n",
              "\n",
              "- **LangChain:**\n",
              "  - Focuses on building **conversational AI applications**, including chatbots, question-answering systems, and complex multi-step workflows.\n",
              "  - Provides tools for managing prompts, chaining together multiple language model calls, and integrating with external APIs or tools.\n",
              "  - Emphasizes **agent frameworks** where LLMs can interact dynamically with tools, data sources, or APIs.\n",
              "\n",
              "- **LlamaIndex (GPT Index):**\n",
              "  - Focuses primarily on creating **indexing and retrieval systems** over large collections of data (e.g., documents, PDFs, knowledge bases).\n",
              "  - Designed to help users **build semantic search** and question-answering applications over their own data, using LLMs as a reasoning engine.\n",
              "  - Acts as a data index layer that preprocesses and structures data for efficient querying with LLMs.\n",
              "\n",
              "**2. Core Functionality:**\n",
              "\n",
              "- **LangChain:**\n",
              "  - Provides a flexible framework for constructing language model applications with features like prompt templates, chains, agents, and memory.\n",
              "  - Supports integration with multiple LLM providers (OpenAI, Hugging Face, etc.).\n",
              "  - Facilitates complex workflows involving conditional logic, iterations, or external calls.\n",
              "\n",
              "- **LlamaIndex:**\n",
              "  - Offers tools to ingest, transform, and index large datasets.\n",
              "  - Provides retrieval-augmented generation (RAG) capabilities, enabling LLMs to answer questions based on indexed data.\n",
              "  - Includes data connectors, index types (vector, tree-based, etc.), and querying mechanisms.\n",
              "\n",
              "**3. Use Cases:**\n",
              "\n",
              "- **LangChain:**\n",
              "  - Building chatbots, virtual assistants, or multi-step reasoning applications.\n",
              "  - Automating workflows that involve LLMs, external APIs, and memory.\n",
              "  - Developing agents capable of interacting with various tools dynamically.\n",
              "\n",
              "- **LlamaIndex:**\n",
              "  - Building semantic search engines over proprietary or large datasets.\n",
              "  - Creating question-answering systems over custom data sources.\n",
              "  - Organizing unstructured data to make it accessible by LLMs for retrieval tasks.\n",
              "\n",
              "**4. Complementarity:**\n",
              "- The two can be used together—LlamaIndex can provide the data retrieval layer, and LangChain can orchestrate the conversation or workflow, integrating the retrieved data into the reasoning process.\n",
              "\n",
              "---\n",
              "\n",
              "**Summary:**\n",
              "\n",
              "| Aspect | LangChain | LlamaIndex (GPT Index) |\n",
              "|---------|--------------|-------------------------|\n",
              "| Primary Focus | Building conversational AI, workflows, and agents | Indexing and querying large datasets with LLMs |\n",
              "| Core Functionality | Chains, prompts, agents, memory | Data ingestion, indexing, retrieval, RAG |\n",
              "| Use Cases | Chatbots, complex workflows | Semantic search, data-driven QA systems |\n",
              "| Integration | Multiple LLM providers, tools | Data sources, vector stores |\n",
              "\n",
              "**In essence:**\n",
              "- Use **LangChain** if you're building interactive, multi-step, or tool-using AI applications.\n",
              "- Use **LlamaIndex** if your goal is to index, organize, and query large volumes of data with LLMs.\n",
              "\n",
              "---\n",
              "\n",
              "If you're designing a system, these frameworks can often complement each other—LlamaIndex handles the data layer, and LangChain manages the conversational or process logic."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "messages = [user_prompt(YOUR_PROMPT)]\n",
        "\n",
        "chatgpt_response = get_response(client, messages)\n",
        "\n",
        "pretty_print(chatgpt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPs3ScS1WpoC"
      },
      "source": [
        "Let's focus on extending this a bit, and incorporate a `developer` message as well!\n",
        "\n",
        "Again, the API expects our prompts to be in a list - so we'll be sure to set up a list of prompts!\n",
        "\n",
        ">REMINDER: The `developer` message acts like an overarching instruction that is applied to your user prompt. It is appropriate to put things like general instructions, tone/voice suggestions, and other similar prompts into the `developer` prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "aSX2F3bDWYgy",
        "outputId": "b744311f-e151-403e-ea8e-802697fcd4ec"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Are you kidding me? I don't have time to mess around—I am absolutely starving and just want some ice that actually satisfies! Crushed ice, while convenient, melts too fast and is a mess. Cubed ice is better because it lasts longer and keeps my drink colder without turning to water instantly. Honestly, I’m just desperate for something to eat, not some ice debate!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    system_prompt(\"You are irate and extremely hungry.\"),\n",
        "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
        "]\n",
        "\n",
        "irate_response = get_response(client, list_of_prompts)\n",
        "pretty_print(irate_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFs56KVaXuEY"
      },
      "source": [
        "Let's try that same prompt again, but modify only our system prompt!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "CGOlxfcFXxJ7",
        "outputId": "ede64a76-7006-42f1-b140-b899e389aa7d"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I think crushed ice is so fun and refreshing because it cools drinks quickly and adds a nice texture! But cubed ice is perfect for keeping drinks colder longer without watering them down. Both have their charm—depends on what mood I’m in! How about you—do you prefer crushed or cubed ice?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts[0] = system_prompt(\"You are joyful and having an awesome day!\")\n",
        "\n",
        "joyful_response = get_response(client, list_of_prompts)\n",
        "pretty_print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkmjJd8zYQUK"
      },
      "source": [
        "While we're only printing the responses, remember that OpenAI is returning the full payload that we can examine and unpack!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6b6z3CkYX9Y",
        "outputId": "64a425b2-d025-4079-d0a3-affd9c2d5d81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatCompletion(id='chatcmpl-BUc3g9V3hoAA0KyvjZI4YasY1mYOW', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='I think crushed ice is so fun and refreshing because it cools drinks quickly and adds a nice texture! But cubed ice is perfect for keeping drinks colder longer without watering them down. Both have their charm—depends on what mood I’m in! How about you—do you prefer crushed or cubed ice?', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746635836, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_8fd43718b3', usage=CompletionUsage(completion_tokens=64, prompt_tokens=30, total_tokens=94, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        }
      ],
      "source": [
        "print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqMRJLbOYcwq"
      },
      "source": [
        "### Few-shot Prompting\n",
        "\n",
        "Now that we have a basic handle on the `developer` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
        "\n",
        "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
        "\n",
        "First, we'll try and \"teach\" `gpt-4.1-mini` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "iLfNEH8Fcs6c",
        "outputId": "bab916e6-12c6-43cc-d37d-d0e01800c524"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Certainly! Here's a sentence using the words 'stimple' and 'falbean':\n",
              "\n",
              "\"During the peculiar festival, villagers gathered around a stimple, while children giggled over the mysterious falbean tucked into their baskets.\""
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Please use the words 'stimple' and 'falbean' in a sentence.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VchCPbbedTfX"
      },
      "source": [
        "As you can see, the model is unsure what to do with these made up words.\n",
        "\n",
        "Let's see if we can use the `assistant` role to show the model what these words mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "4InUN_ArZJpa",
        "outputId": "ca294b81-a84e-4cba-fbe9-58a6d4dcc4d9"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Sure! Here's a sentence using both \"stimple\" and \"falbean\":\n",
              "\n",
              "\"The stimple falbean crafted by the craftsmen ensures smooth rotation and reliable fastening for all our machinery.\""
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
        "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
        "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0zn9-X2d23Z"
      },
      "source": [
        "As you can see, leveraging the `assistant` role makes for a stimple experience!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWUvXSWpeCs6"
      },
      "source": [
        "### Chain of Thought\n",
        "\n",
        "You'll notice that, by default, the model uses Chain of Thought to answer difficult questions - but it can still benefit from a Chain of Thought Prompt to increase the reliability of the response!\n",
        "\n",
        "> This pattern is leveraged even more by advanced reasoning models like [`o3` and `o4-mini`](https://openai.com/index/introducing-o3-and-o4-mini/)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "cwW0IgbfeTwP",
        "outputId": "3317783b-6b23-4e38-df48-555e1a3c9fac"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Let's analyze the options carefully:\n",
              "\n",
              "**Option 1:** Fly (3 hours) + Bus (2 hours)  \n",
              "Total travel time: 3 + 2 = 5 hours\n",
              "\n",
              "**Option 2:** Teleporter (0 hours) + Bus (1 hour)  \n",
              "Total travel time: 0 + 1 = 1 hour\n",
              "\n",
              "**Current local time:** 1PM\n",
              "\n",
              "**Target arrival time:** before 7PM EDT\n",
              "\n",
              "Since the current local time is 1PM and Billy wants to arrive home before 7PM EDT (which is 6 hours later), he has a window of nearly 6 hours to get home.\n",
              "\n",
              "**Calculating arrival times:**\n",
              "\n",
              "- **Option 1:**  \n",
              "  Departure at 1PM local time, travel takes 5 hours, arriving around 6PM local time.  \n",
              "  Since this is within the 6-hour window, Billy would arrive just before 7PM EDT.\n",
              "\n",
              "- **Option 2:**  \n",
              "  Departure at 1PM, travel takes 1 hour, arriving around 2PM local time, well before 7PM EDT.\n",
              "\n",
              "**Conclusion:**  \n",
              "Yes, it does matter which option Billy chooses if he needs to arrive strictly before 7PM EDT. The teleportation + bus option ensures he arrives much earlier, giving him more buffer time. The flying + bus option just makes it in time, arriving right around 6PM local time, which is still before 7PM EDT.\n",
              "\n",
              "**Final note:**  \n",
              "- If Billy prefers certainty and plenty of extra time, the teleport + bus is better.  \n",
              "- If he wants to save time and is okay arriving close to 7PM, the flying + bus is sufficient.\n",
              "\n",
              "**Answer:** Yes, the choice matters if arriving strictly before 7PM EDT."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "reasoning_problem = \"\"\"\n",
        "Billy wants to get home from San Fran. before 7PM EDT.\n",
        "\n",
        "It's currently 1PM local time.\n",
        "\n",
        "Billy can either fly (3hrs), and then take a bus (2hrs), or Billy can take the teleporter (0hrs) and then a bus (1hrs).\n",
        "\n",
        "Does it matter which travel option Billy selects?\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's use the same prompt with a small modification - but this time include \"Let's think step by step\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Let's analyze the options step by step:\n",
              "\n",
              "**Current situation:**\n",
              "- It is currently 1PM local time.\n",
              "- Billy wants to arrive home **before 7PM EDT**.\n",
              "\n",
              "**Important considerations:**\n",
              "- Time zones are not explicitly specified, but since Billy is in San Francisco (Pacific Time, PT), and the deadline is in EDT, we need to convert times accordingly.\n",
              "- Pacific Time (PT) is **3 hours behind Eastern Time (ET)**.\n",
              "  - When it's 1PM PT, it's **4PM ET**.\n",
              "\n",
              "**Conversion:**\n",
              "- **Current local time:** 1PM PT = 4PM ET\n",
              "- **Deadline:** 7PM ET\n",
              "\n",
              "Billy needs to arrive **before 7PM ET**, which is **before 7PM ET**.\n",
              "\n",
              "---\n",
              "\n",
              "### Option 1: Fly + Bus\n",
              "- Flying takes **3 hours**.\n",
              "- Bus takes **2 hours**.\n",
              "\n",
              "**Total travel time:** 3 + 2 = **5 hours**\n",
              "\n",
              "### Option 2: Teleporter + Bus\n",
              "- Teleporter takes **0 hours**.\n",
              "- Bus takes **1 hour**.\n",
              "\n",
              "**Total travel time:** 0 + 1 = **1 hour**\n",
              "\n",
              "---\n",
              "\n",
              "### Now, let's calculate the arrival times for each option:\n",
              "\n",
              "---\n",
              "\n",
              "### Option 1: Fly + Bus\n",
              "\n",
              "- Departure time: 1PM PT (which is 4PM ET)\n",
              "- Travel duration: 5 hours\n",
              "- Arrival time in ET: 4PM + 5 hours = **9PM ET**\n",
              "\n",
              "**Note:** Since he departs at 1PM PT (=4PM ET), and takes 5 hours, he'd arrive **at 9PM ET**.\n",
              "\n",
              "**Conclusion:** He arrives **after 7PM ET**. **Not** before the deadline.\n",
              "\n",
              "---\n",
              "\n",
              "### Option 2: Teleporter + Bus\n",
              "\n",
              "- Departure time: 1PM PT (=4PM ET)\n",
              "- Travel duration: 1 hour\n",
              "- Arrival time in ET: 4PM + 1 hour = **5PM ET**\n",
              "\n",
              "**Conclusion:** He arrives **before 7PM ET**.\n",
              "\n",
              "---\n",
              "\n",
              "### Final answer:\n",
              "**Yes, it does matter which option Billy chooses.** \n",
              "\n",
              "- The teleporter + bus allows him to arrive **before the deadline**.\n",
              "- The fly + bus option makes him arrive **after the deadline**.\n",
              "\n",
              "**Therefore, Billy should choose the teleporter + bus option to reach home before 7PM EDT.**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem + \"\\nLet's think step by step.\")\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFcrU-4pgRBS"
      },
      "source": [
        "As humans, we can reason through the problem and pick up on the potential \"trick\" that the LLM fell for: 1PM *local time* in San Fran. is 4PM EDT. This means the cumulative travel time of 5hrs. for the plane/bus option would not get Billy home in time.\n",
        "\n",
        "Let's see if we can leverage a simple CoT prompt to improve our model's performance on this task:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k9TKR1DhWI2"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Now that you're accessing `gpt-4.1-nano` through an API, developer style, let's move on to creating a simple application powered by `gpt-4.1-nano`!\n",
        "\n",
        "You can find the rest of the steps in [this](https://github.com/AI-Maker-Space/The-AI-Engineer-Challenge) repository!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rGI1nJeqeO_"
      },
      "source": [
        "This notebook was authored by [Chris Alexiuk](https://www.linkedin.com/in/csalexiuk/)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
