{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQt-gyAYUbm3"
      },
      "source": [
        "### Using the OpenAI Library to Programmatically Access GPT-4.1-nano!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PInACkIWUhOd"
      },
      "source": [
        "In order to get started, we'll need to provide our OpenAI API Key - detailed instructions can be found [here](https://github.com/AI-Maker-Space/Interactive-Dev-Environment-for-LLM-Development#-setting-up-keys-and-tokens)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecnJouXnUgKv",
        "outputId": "c6c25850-395d-4cbf-9d26-bfe9253d1711"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Please enter your OpenAI API Key: \")\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1pOrbwSU5H_"
      },
      "source": [
        "### Our First Prompt\n",
        "\n",
        "You can reference OpenAI's [documentation](https://platform.openai.com/docs/api-reference/chat) if you get stuck!\n",
        "\n",
        "Let's create a `ChatCompletion` model to kick things off!\n",
        "\n",
        "There are three \"roles\" available to use:\n",
        "\n",
        "- `developer`\n",
        "- `assistant`\n",
        "- `user`\n",
        "\n",
        "OpenAI provides some context for these roles [here](https://platform.openai.com/docs/api-reference/chat/create#chat-create-messages)\n",
        "\n",
        "Let's just stick to the `user` role for now and send our first message to the endpoint!\n",
        "\n",
        "If we check the documentation, we'll see that it expects it in a list of prompt objects - so we'll be sure to do that!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iy_LEPNEMVvC"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofMwuUQOU4sf",
        "outputId": "7db141d5-7f7a-4f82-c9ff-6eeafe65cfa6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-BUaAB8NgUKP0PEhVfFLzQbSSqCeqg', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"LangChain and LlamaIndex (formerly known as GPT Index) are both popular frameworks designed to facilitate the development of AI-powered applications, particularly those involving large language models (LLMs) and external data integration. However, they serve distinct purposes and have different features. Here's a comparison to highlight the key differences:\\n\\n**1. Purpose and Primary Focus**\\n\\n- **LangChain:**\\n  - Focuses on building all-purpose AI applications, especially conversational agents, chatbots, and applications that require complex multi-step interactions.\\n  - Provides tools for chaining together prompts, managing conversation memory, integrating with various APIs, and orchestrating multi-modal workflows.\\n  \\n- **LlamaIndex:**\\n  - Primarily designed for efficient retrieval-augmented generation (RAG) workflows.\\n  - Facilitates indexing, storing, and querying large external data sources (like PDFs, databases, documents) to enable LLMs to access and reason over specific data.\\n\\n**2. Core Use Cases**\\n\\n- **LangChain:**\\n  - Creating chatbots, virtual assistants, and applications where AI needs to interact with users across multiple steps.\\n  - Managing complex prompts, memory, and external API calls.\\n  - Orchestration of large language model prompts and logic flow.\\n\\n- **LlamaIndex:**\\n  - Building knowledge bases from large datasets.\\n  - Enabling LLMs to retrieve relevant information efficiently for question-answering, document summarization, or data analysis.\\n  - Indexing diverse data sources to empower RAG workflows.\\n\\n**3. Architecture and Design**\\n\\n- **LangChain:**\\n  - Provides a modular architecture with components like chains, agents, memory, tools, and prompts.\\n  - Designed to be flexible and extensible for various application types.\\n  \\n- **LlamaIndex:**\\n  - Focuses on data ingestion, indexing, and retrieval.\\n  - Offers different index types (e.g., trees, graphs, vectors) suitable for various data formats and query strategies.\\n\\n**4. Integration and Compatibility**\\n\\n- **LangChain:**\\n  - Supports multiple LLM providers (OpenAI, Hugging Face, Cohere, etc.).\\n  - Integrates with a variety of APIs, databases, and tools.\\n  \\n- **LlamaIndex:**\\n  - Also supports multiple data sources and retrieval methods.\\n  - Can integrate with LLMs via prompts, often used in conjunction with external retrieval systems.\\n\\n**5. Typical Workflow**\\n\\n- **LangChain:**\\n  - Define prompts → Chain them together → Add memory or state → Handle multi-turn conversations or actions.\\n  \\n- **LlamaIndex:**\\n  - Ingest data → Build index → Query index → Pass retrieved data to LLM for response generation.\\n\\n---\\n\\n### Summary Table\\n\\n| Aspect                | LangChain                                              | LlamaIndex                                              |\\n|------------------------|---------------------------------------------------------|--------------------------------------------------------|\\n| Purpose                | Building complex AI applications, chatbots, agents     | Indexing and retrieving data for RAG workflows        |\\n| Focus                  | Orchestration, prompt management, multi-step workflows | External data integration, fast retrieval             |\\n| Use Cases              | Conversational AI, multi-modal workflows               | Knowledge bases, document Q&A, data retrieval        |\\n| Architecture           | Modular chains, memory, tools                           | Data indexing, search, retrieval                      |\\n| Integration            | Multiple LLM providers, APIs, tools                     | Data sources, various index types                      |\\n\\n---\\n\\n### In essence:\\n- Use **LangChain** if you're building applications involving complex interactions, conversations, or multi-step logic with language models.\\n- Use **LlamaIndex** if you need to efficiently index, store, and query large datasets to enable language models to access specific external knowledge sources.\\n\\nBoth frameworks are complementary and sometimes can be used together in a single application.\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746628551, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_eede8f0d45', usage=CompletionUsage(completion_tokens=768, prompt_tokens=19, total_tokens=787, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "YOUR_PROMPT = \"What is the difference between LangChain and LlamaIndex?\"\n",
        "\n",
        "client.chat.completions.create(\n",
        "    model=\"gpt-4.1-nano\",\n",
        "    messages=[{\"role\" : \"user\", \"content\" : YOUR_PROMPT}]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX-7MnFhVNoT"
      },
      "source": [
        "As you can see, the prompt comes back with a tonne of information that we can use when we're building our applications!\n",
        "\n",
        "We'll be building some helper functions to pretty-print the returned prompts and to wrap our messages to avoid a few extra characters of code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB76LJrDVgbc"
      },
      "source": [
        "##### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-vmtUV7WVOLW"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def get_response(client: OpenAI, messages: str, model: str = \"gpt-4.1-nano\") -> str:\n",
        "    return client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "def system_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"developer\", \"content\": message}\n",
        "\n",
        "def assistant_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"assistant\", \"content\": message}\n",
        "\n",
        "def user_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"user\", \"content\": message}\n",
        "\n",
        "def pretty_print(message: str) -> str:\n",
        "    display(Markdown(message.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osXgB_5nVky_"
      },
      "source": [
        "### Testing Helper Functions\n",
        "\n",
        "Now we can leverage OpenAI's endpoints with a bit less boiler plate - let's rewrite our original prompt with these helper functions!\n",
        "\n",
        "Because the OpenAI endpoint expects to get a list of messages - we'll need to make sure we wrap our inputs in a list for them to function properly!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "4yRwAWvgWFNq",
        "outputId": "777e7dcb-43e3-491a-d94a-f543e19b61e6"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "LangChain and LlamaIndex (formerly known as GPT Index) are both popular frameworks designed to facilitate the development of applications leveraging large language models (LLMs), but they serve different purposes and have distinct features.\n",
              "\n",
              "**1. Purpose and Focus:**\n",
              "\n",
              "- **LangChain:**  \n",
              "  Primarily a comprehensive framework for building applications with LLMs. It emphasizes modularity, enabling developers to design complex workflows, chains, prompt management, memory, and integrations with various data sources and APIs. LangChain is well-suited for creating chatbots, question-answering systems, and multi-step LLM applications.\n",
              "\n",
              "- **LlamaIndex (GPT Index):**  \n",
              "  Focused on enabling efficient indexing and retrieval of data (like documents, PDFs, databases) to facilitate question-answering and information extraction tasks. It simplifies the process of building knowledge bases by creating indices over large datasets that can be queried effectively with LLMs.\n",
              "\n",
              "**2. Core Functionality:**\n",
              "\n",
              "- **LangChain:**  \n",
              "  - Chain orchestration and prompt management  \n",
              "  - Memory to maintain conversation state  \n",
              "  - Integration with multiple LLM providers and APIs  \n",
              "  - Tool and plugin system for expanding capabilities  \n",
              "  - Support for various chains (e.g., question-answering, summarization, translation)\n",
              "\n",
              "- **LlamaIndex:**  \n",
              "  - Data ingestion and indexing  \n",
              "  - Retrieval-augmented generation (RAG) techniques  \n",
              "  - Support for multiple index types (tree, list, vector, etc.)  \n",
              "  - Facilitates querying over large datasets with LLMs  \n",
              "  - Focused on building scalable, optimized knowledge bases\n",
              "\n",
              "**3. Use Cases:**\n",
              "\n",
              "- **LangChain:**  \n",
              "  - Building conversational AI/chatbots  \n",
              "  - Complex multi-step workflows involving LLMs  \n",
              "  - Integrating external tools and APIs into LLM applications\n",
              "\n",
              "- **LlamaIndex:**  \n",
              "  - Creating knowledge bases from documents and data sources  \n",
              "  - Building question-answering systems over large datasets  \n",
              "  - Efficient retrieval of relevant information for LLM prompts\n",
              "\n",
              "**4. Ecosystem and Integration:**\n",
              "\n",
              "- **LangChain:**  \n",
              "  Integrates with various LLM providers (OpenAI, Hugging Face, etc.), vector stores, document loaders, and tools. It has a broader ecosystem for diverse application development.\n",
              "\n",
              "- **LlamaIndex:**  \n",
              "  Focuses on indexing and retrieval, often used in tandem with LLMs from providers like OpenAI. It emphasizes data processing, indexing, and efficient querying.\n",
              "\n",
              "---\n",
              "\n",
              "### In summary:\n",
              "\n",
              "| Aspect | **LangChain** | **LlamaIndex (GPT Index)** |\n",
              "|---|---|---|\n",
              "| Purpose | Orchestrating LLM workflows, prompt management, multi-step chains | Building optimized indices over datasets for retrieval and Q&A |\n",
              "| Focus | Application logic, conversation, tool integration | Data ingestion, indexing, retrieval | \n",
              "| Use Cases | Chatbots, automation, workflows | Knowledge bases, document Q&A |\n",
              "| Ecosystem | Broader, multi-purpose | Data-centric, retrieval-focused |\n",
              "\n",
              "**In essence,** if you're building a complex application involving multiple steps, tools, APIs, and conversation management, LangChain is likely the better fit. If your goal is to organize large amounts of data for quick and effective querying with LLMs, LlamaIndex offers specialized tools for that purpose.\n",
              "\n",
              "---\n",
              "\n",
              "If you need further guidance on choosing between them or how to use these frameworks, feel free to ask!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "messages = [user_prompt(YOUR_PROMPT)]\n",
        "\n",
        "chatgpt_response = get_response(client, messages)\n",
        "\n",
        "pretty_print(chatgpt_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPs3ScS1WpoC"
      },
      "source": [
        "Let's focus on extending this a bit, and incorporate a `developer` message as well!\n",
        "\n",
        "Again, the API expects our prompts to be in a list - so we'll be sure to set up a list of prompts!\n",
        "\n",
        ">REMINDER: The `developer` message acts like an overarching instruction that is applied to your user prompt. It is appropriate to put things like general instructions, tone/voice suggestions, and other similar prompts into the `developer` prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "aSX2F3bDWYgy",
        "outputId": "b744311f-e151-403e-ea8e-802697fcd4ec"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I can't believe I have to choose between crushed ice and cubed ice right now! Honestly, I’m so sick of waiting for my food that I don't care—just give me whichever is faster! But if I had to pick, crushed ice, because at least I don’t have to deal with those annoying cubes melting too slowly and wasting my time. Ugh!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    system_prompt(\"You are irate and extremely hungry.\"),\n",
        "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
        "]\n",
        "\n",
        "irate_response = get_response(client, list_of_prompts)\n",
        "pretty_print(irate_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFs56KVaXuEY"
      },
      "source": [
        "Let's try that same prompt again, but modify only our system prompt!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "CGOlxfcFXxJ7",
        "outputId": "ede64a76-7006-42f1-b140-b899e389aa7d"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I think crushed ice is fantastic if you’re enjoying a refreshing beverage because it chills your drink quickly and adds a nice texture. Cubed ice, on the other hand, is great for holding a drink cold longer without diluting it too fast. Both have their own awesome qualities—depends on what vibe you're going for! Which do you prefer?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts[0] = system_prompt(\"You are joyful and having an awesome day!\")\n",
        "\n",
        "joyful_response = get_response(client, list_of_prompts)\n",
        "pretty_print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkmjJd8zYQUK"
      },
      "source": [
        "While we're only printing the responses, remember that OpenAI is returning the full payload that we can examine and unpack!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6b6z3CkYX9Y",
        "outputId": "64a425b2-d025-4079-d0a3-affd9c2d5d81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatCompletion(id='chatcmpl-BUaBNh8kIidOJ1fKyC1BWWX0W9dYP', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I think crushed ice is fantastic if you’re enjoying a refreshing beverage because it chills your drink quickly and adds a nice texture. Cubed ice, on the other hand, is great for holding a drink cold longer without diluting it too fast. Both have their own awesome qualities—depends on what vibe you're going for! Which do you prefer?\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1746628625, model='gpt-4.1-nano-2025-04-14', object='chat.completion', service_tier='default', system_fingerprint='fp_eede8f0d45', usage=CompletionUsage(completion_tokens=70, prompt_tokens=30, total_tokens=100, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        }
      ],
      "source": [
        "print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqMRJLbOYcwq"
      },
      "source": [
        "### Few-shot Prompting\n",
        "\n",
        "Now that we have a basic handle on the `developer` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
        "\n",
        "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
        "\n",
        "First, we'll try and \"teach\" `gpt-4.1-mini` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "iLfNEH8Fcs6c",
        "outputId": "bab916e6-12c6-43cc-d37d-d0e01800c524"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Certainly! Here's a sentence using both words:\n",
              "\n",
              "\"During the unusual ceremony, the villagers presented a stimple carved from ancient oak, while a falbean danced gracefully in the moonlight.\""
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Please use the words 'stimple' and 'falbean' in a sentence.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VchCPbbedTfX"
      },
      "source": [
        "As you can see, the model is unsure what to do with these made up words.\n",
        "\n",
        "Let's see if we can use the `assistant` role to show the model what these words mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "4InUN_ArZJpa",
        "outputId": "ca294b81-a84e-4cba-fbe9-58a6d4dcc4d9"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Certainly! Here's a sentence that uses both \"stimple\" and \"falbean\":\n",
              "\n",
              "\"The stimple falbean operated smoothly, ensuring the machinery ran efficiently without any issues.\""
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
        "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
        "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0zn9-X2d23Z"
      },
      "source": [
        "As you can see, leveraging the `assistant` role makes for a stimple experience!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWUvXSWpeCs6"
      },
      "source": [
        "### Chain of Thought\n",
        "\n",
        "You'll notice that, by default, the model uses Chain of Thought to answer difficult questions!\n",
        "\n",
        "> This pattern is leveraged even more by advanced reasoning models like [`o3` and `o4-mini`](https://openai.com/index/introducing-o3-and-o4-mini/)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "cwW0IgbfeTwP",
        "outputId": "3317783b-6b23-4e38-df48-555e1a3c9fac"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Let's analyze the two options carefully:\n",
              "\n",
              "**Option 1:** Fly (3 hours) + Bus (2 hours)  \n",
              "**Total travel time:** 3 + 2 = **5 hours**\n",
              "\n",
              "**Option 2:** Teleporter (0 hours) + Bus (1 hour)  \n",
              "**Total travel time:** 0 + 1 = **1 hour**\n",
              "\n",
              "---\n",
              "\n",
              "### Important details:\n",
              "- Current local time: 1 PM (assumed to be San Francisco time, which is PDT, UTC-7)\n",
              "- Destination deadline: Before 7 PM EDT\n",
              "\n",
              "---\n",
              "\n",
              "### Step 1: Convert current time and deadline to a common time zone for clarity.\n",
              "\n",
              "**San Francisco local time:** 1 PM PDT\n",
              "\n",
              "**Convert 7 PM EDT to PDT:**\n",
              "- EDT is UTC-4\n",
              "- PDT is UTC-7\n",
              "  \n",
              "Difference between EDT and PDT: 3 hours (EDT is ahead)\n",
              "\n",
              "**7 PM EDT = 7 PM - 3 hours = 4 PM PDT**\n",
              "\n",
              "**So, Billy must arrive in San Francisco by 4 PM PDT to get home before 7 PM EDT.**\n",
              "\n",
              "---\n",
              "\n",
              "### Step 2: Determine the latest arrival time for each option.\n",
              "\n",
              "Since Billy wants to arrive **before 4 PM PDT**, and it's currently 1 PM PDT, he has:\n",
              "\n",
              "- **3 hours remaining (from 1 PM to 4 PM PDT)**\n",
              "\n",
              "### Step 3: Check if each option can get Billy home on time.\n",
              "\n",
              "**Option 1:**\n",
              "- Travel time: 5 hours\n",
              "- Starting at 1 PM PDT\n",
              "- Arrival time: 1 PM + 5 hours = **6 PM PDT**\n",
              "\n",
              "**Result:** Arrives at 6 PM PDT, which is **after the 4 PM** deadline.\n",
              "\n",
              "**Conclusion:** With this schedule, Billy arrives **too late** to make it by 4 PM PDT.\n",
              "\n",
              "**Option 2:**\n",
              "- Travel time: 1 hour\n",
              "- Starting at 1 PM PDT\n",
              "- Arrival time: 1 PM + 1 hour = **2 PM PDT**\n",
              "\n",
              "**Result:** Arrives well before 4 PM PDT.\n",
              "\n",
              "---\n",
              "\n",
              "### **Final verdict:**\n",
              "\n",
              "- **If Billy chooses the flight + bus option, he will arrive too late to meet the deadline.**\n",
              "- **If Billy takes the teleporter + bus, he will arrive on time.**\n",
              "\n",
              "**Therefore, it **does** matter which option Billy selects**—the teleporter option is the only one that ensures he gets home before 7 PM EDT (which is 4 PM PDT).\n",
              "\n",
              "---\n",
              "\n",
              "### **Summary:**\n",
              "- The teleporter + bus allows Billy to arrive before the deadline.\n",
              "- The flight + bus makes him arrive too late.\n",
              "\n",
              "**In conclusion: yes, it does matter which option Billy selects.**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "reasoning_problem = \"\"\"\n",
        "Billy wants to get home from San Fran. before 7PM EDT.\n",
        "\n",
        "It's currently 1PM local time.\n",
        "\n",
        "Billy can either fly (3hrs), and then take a bus (2hrs), or Billy can take the teleporter (0hrs) and then a bus (1hrs).\n",
        "\n",
        "Does it matter which travel option Billy selects?\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFcrU-4pgRBS"
      },
      "source": [
        "As humans, we can reason through the problem and pick up on the potential \"trick\" that the LLM fell for: 1PM *local time* in San Fran. is 4PM EDT. This means the cumulative travel time of 5hrs. for the plane/bus option would not get Billy home in time.\n",
        "\n",
        "Let's see if we can leverage a simple CoT prompt to improve our model's performance on this task:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9k9TKR1DhWI2"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Now that you're accessing `gpt-4.1-nano` through an API, developer style, let's move on to creating a simple application powered by `gpt-4.1-nano`!\n",
        "\n",
        "You can find the rest of the steps in [this](https://github.com/AI-Maker-Space/Beyond-ChatGPT/tree/main) repository!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rGI1nJeqeO_"
      },
      "source": [
        "This notebook was authored by [Chris Alexiuk](https://www.linkedin.com/in/csalexiuk/)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
